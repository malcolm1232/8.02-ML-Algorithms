{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "*  In this practical, we will learn to train decision tree classifiers and regressors and evaluate their performance <br>\n",
    "*  We will first build the trees without constraints and use their performance as the baseline for comparison <br>\n",
    "*  We will then set regularization hyperparameters and use grid search with cross validation to find the best-performing model <br>\n",
    "*  We will use the telecom customer churn dataset for these exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision tree classifier with customer churn dataset\n",
    "*  We will train a decision tree classifier on the telecom customer churn dataset <br>\n",
    "*  The objective is to predict customer churn (true or false) based on the customers' service usage attributes in the dataset <br>\n",
    "(Some parts of this section are adopted from Reference [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, let's explore the dataset and perform necessary preparation for the machine learning task at hand\n",
    "\n",
    "* Import the customer churn data\n",
    "* Remove any columns that are unlikely to be useful for prediction\n",
    "* Encode data types as appropriate\n",
    "* Split the dataset into the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Orange_Telecom_Churn_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the attributes (features) and their data types? Any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 20 data insances and take note of the four non-numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data range of numeric features <br>\n",
    "As decision trees can handle data of any scale, we seldom need to perform data scaling for training decision tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the features, `phone_number` is unique to each customer and should not be used as a predictor <br>\n",
    "Also, it is unlikely that `area_code` or `state` would be desired, unless there is some reason to assume that the model has a very specific geographic factor <br>\n",
    "We can drop these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['state', 'phone_number', 'area_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are numeric except `intl_plan` and `voice_mail_plan` that need to be boolean encoded (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.intl_plan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.voice_mail_plan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['intl_plan', 'voice_mail_plan']:\n",
    "    data[col] = data[col].replace('yes', True).replace('no', False).astype(np.bool)\n",
    "      \n",
    "data[['intl_plan', 'voice_mail_plan']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the target variable `churned` <br>\n",
    "Split the dataset into the training and test sets (decide if a stratified split should be used or not, based on the distribution) <br>\n",
    "After the split, examine the distribution of the target variable in the training and test sets to be assured that the two classes are proportionally represented in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.churned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.churned.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is skewed at ~85% towards non-churned customers <br>\n",
    "This will be an important factor to consider when evaluating the trained model <br>\n",
    "Given the skew in the target variable, let's split the data into the training and test sets which are stratified by the `churned` values (target labels) <br>\n",
    "We will use `StratefiedShuffleSplit()`, instead of `train_test_split()` which does the split randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Separate feature columns and the target column\n",
    "feature_cols = [x for x in data.columns if x != 'churned']\n",
    "\n",
    "# Split the data into two parts with 30% samples in the test set\n",
    "\n",
    "# This creates a generator\n",
    "strat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Get the index values from the generator\n",
    "train_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['churned']))\n",
    "\n",
    "# Create the data sets\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'churned']\n",
    "\n",
    "X_test = data.loc[test_idx, feature_cols]\n",
    "y_test = data.loc[test_idx, 'churned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the number of each target label in the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of the churned and non-churned numbers in the test set (we need to take these into consideration when evaluating the performance metrics of the trained classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is done <br>\n",
    "Now, build the decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decision tree without constraints\n",
    "* Fit a decision tree classifier without set limits to constrain its growth (i.e. no regularization) <br>\n",
    "* This is almost certain to lead to overfitting <br>\n",
    "* Determine how many nodes there are and what the depth of this (very large) tree is <br>\n",
    "* Using this tree, measure the prediction error in the training and test sets <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-09T23:59:21.711217Z",
     "start_time": "2017-05-09T23:59:21.651488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf = dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of nodes and the depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.tree_.node_count, dt_clf.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree model also tells us the relative importances of the features through the `feature_importances_` variable<br>\n",
    "Which attributes are the biggest predictors of customer churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "feature_imp = pd.Series(dt_clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = feature_imp.plot(kind='bar')\n",
    "ax.set(ylabel='Relative Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to return various error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-09T23:59:21.730535Z",
     "start_time": "2017-05-09T23:59:21.723077Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def measure_error(y_true, y_pred, label):\n",
    "    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n",
    "                      'precision': precision_score(y_true, y_pred),\n",
    "                      'recall': recall_score(y_true, y_pred),\n",
    "                      'f1': f1_score(y_true, y_pred)},\n",
    "                      name=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the performance of the trained model on the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error on the training and test data sets\n",
    "y_train_pred = dt_clf.predict(X_train)\n",
    "y_test_pred = dt_clf.predict(X_test)\n",
    "\n",
    "train_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n",
    "                                   measure_error(y_test, y_test_pred, 'test')],\n",
    "                                   axis=1)\n",
    "\n",
    "train_test_full_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  What do you think is going on here based on the difference in training and test accuracies? <br>\n",
    "*  The decision tree predicts far better on the training data than the test data, which is a sign of overfitting <br>\n",
    "*  Also notice the perfect scores on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also take a look at the confusion matrix on the test set prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "sns.set_context('talk')\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_yticklabels(labels);\n",
    "ax.set_ylabel('Actual');\n",
    "ax.set_xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we interpret this confusion matrix? <br>\n",
    "Given the skewed distribution of the target classes, which metric do you think is the most important? <br>\n",
    "What could be a better result given the objective of this machine learning task (i.e. to predict customer churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although recall is not high for the customers who churned (positive class) since their number is quite small, we are still doing better than random guessing (classification accuracy of 91% vs 86% for random guessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Regularization using grid search with cross validation\n",
    "\n",
    "* Using grid search with cross validation, we try to find a combination of hyperparameters that will produce a decision tree with optimized performance <br>\n",
    "* In this experiment, we will try to find an optimal combination of `max_depth` and `max_features`, both of which are regularization hyperparameters <br>\n",
    "* Find out the number of nodes and the depth of this tree <br>\n",
    "* Measure the errors on the training and test sets and compare them to those obtained from the tree in Section 1.2 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': range(1, dt_clf.tree_.max_depth+1, 2),\n",
    "              'max_features': range(1, len(X_train.columns)+1)}\n",
    "\n",
    "gr_dt_clf = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "gr_dt_clf = gr_dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of nodes and the depth of the best-performing tree based on the combinations of `max_depth` and `max_features` specified by us <br>\n",
    "As we will see, this is a much smaller tree than the unrestricted one obtained in Section 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_clf.best_estimator_.tree_.node_count, gr_dt_clf.best_estimator_.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the best combination of the hyperparameters from the `best_params_` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the performance metrics of the best-performing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gr_dt_clf.predict(X_train)\n",
    "y_test_pred = gr_dt_clf.predict(X_test)\n",
    "\n",
    "train_test_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n",
    "                              measure_error(y_test, y_test_pred, 'test')],\n",
    "                              axis=1)\n",
    "\n",
    "train_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is much improved from the previous one <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the confusion matrix on the test set prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "sns.set_context('talk')\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_yticklabels(labels);\n",
    "ax.set_ylabel('Actual');\n",
    "ax.set_xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not get a better recall score for the churned customers as we hoped for, although the precision is much better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an attempt to address the issue of imbalanced dataset, one technique that we can try is to apply `class_weight` when training the decision tree model\n",
    "\n",
    "For training of decision tree classifiers on imbalanced datasets, interested learners may want to refer to this article: https://machinelearningmastery.com/cost-sensitive-decision-trees-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': range(1, dt_clf.tree_.max_depth+1, 2),\n",
    "              'max_features': range(1, len(X_train.columns)+1)}\n",
    "\n",
    "gr_dt_clf = GridSearchCV(DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "gr_dt_clf = gr_dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_clf.best_estimator_.tree_.node_count, gr_dt_clf.best_estimator_.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the performance metrics of the best-performing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gr_dt_clf.predict(X_train)\n",
    "y_train_pred_prob = gr_dt_clf.predict_proba(X_train)\n",
    "y_test_pred = gr_dt_clf.predict(X_test)\n",
    "y_test_pred_prob = gr_dt_clf.predict_proba(X_test)\n",
    "\n",
    "train_test_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n",
    "                              measure_error(y_test, y_test_pred, 'test')],\n",
    "                              axis=1)\n",
    "\n",
    "train_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is much improved, but at the expense of the precision and the overall accuracy <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, this is the force of precision/recall tradeoff at work<br>\n",
    "Where we should set the threshold depends on the problem at hand and the objective we aim to achieve with the machine learning model that we are building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the confusion matrix for the prediction results on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "sns.set_context('talk')\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_yticklabels(labels);\n",
    "ax.set_ylabel('Actual');\n",
    "ax.set_xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision tree classifiers, as we have learned, the class label assigned to a leaf node (hence the predicted class for all data instances associated with the leaf node) is the class that has the higher probability (i.e. >= 50%) in the leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can change the default threshold (50%) to shift the decision point for classification one way or another, in favor of precision or recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the predicted probability of each training instance being a churn (decision tree's `predict_proba()` method can tell us this), and examine the distribution of churned probabilities versus the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scores = y_train_pred_prob[:, 1]  # we use the positive class (churned) probability as the decision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace(True, 1).replace(False, 0).astype(np.int64)\n",
    "\n",
    "train_predict = pd.DataFrame({'actual': y_train.values,\n",
    "                             'predict': y_train_scores})\n",
    "\n",
    "train_predict.plot.scatter(x='predict', y='actual', s=list(range(2,500)), alpha=0.05, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the precision/recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_curve(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve recall, try to set the decision threshold at 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = (y_train_scores > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = pd.concat([measure_error(y_train, y_train_pred, 'train')],\n",
    "                         axis=1)\n",
    "\n",
    "train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is a good improvement in recall, but at the great expense of the precision and overall accuracy (recall for non-churned customers is badly affected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the shift of decision threshold improve the test results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scores = y_test_pred_prob[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = (y_test_scores > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error = pd.concat([measure_error(y_test, y_test_pred, 'test')],\n",
    "                        axis=1)\n",
    "\n",
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion? Setting of the decision threshold is all about balancing the various performance metrics in line with the objectives of the use case that the machine learning model aims to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision tree regressor\n",
    "\n",
    "* Using the same customer churn dataset, we will first build a decision tree regressor without regularization\n",
    "* We will then use grid search with cross validation to build an optimized decision tree regressor\n",
    "* We will measure the errors on the training and test sets using mean squared error <br>\n",
    "* We will make a plot of actual versus predicted customer churn <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a decision tree regressor without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-09T23:59:28.931614Z",
     "start_time": "2017-05-09T23:59:24.318919Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg = dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg.tree_.node_count, dt_reg.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be the same tree as our first decision tree classifier without constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an overfitted regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on training and test datasets <br>\n",
    "Since this is continuous, we will use mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_train_pred_reg = dt_reg.predict(X_train)\n",
    "y_test_pred_reg  = dt_reg.predict(X_test)\n",
    "\n",
    "train_test_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_reg),\n",
    "                              'test':  mean_squared_error(y_test, y_test_pred_reg)},\n",
    "                              name='MSE').to_frame().T\n",
    "\n",
    "train_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the decision tree classifier, the regressor model can tell us which features are more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use grid search to find an optimized regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': range(1, dt_reg.tree_.max_depth+1, 2),\n",
    "              'max_features': range(1, len(X_train.columns)+1)}\n",
    "\n",
    "gr_dt_reg = GridSearchCV(DecisionTreeRegressor(random_state=42),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "gr_dt_reg = gr_dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes and the maximum depth of the best-performing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_reg.best_estimator_.tree_.node_count, gr_dt_reg.best_estimator_.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dt_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on training and test datasets by the best model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_train_pred_reg = gr_dt_reg.predict(X_train)\n",
    "y_test_pred_reg  = gr_dt_reg.predict(X_test)\n",
    "\n",
    "train_test_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_reg),\n",
    "                              'test':  mean_squared_error(y_test, y_test_pred_reg)},\n",
    "                              name='MSE').to_frame().T\n",
    "\n",
    "train_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get much improved results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of actual vs predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.replace(True, 1).replace(False, 0).astype(np.int64)\n",
    "\n",
    "test_predict = pd.DataFrame({'actual': y_test.values,\n",
    "                             'predict': y_test_pred_reg})\n",
    "\n",
    "test_predict.plot.scatter(x='predict', y='actual', s=list(range(2,500)), alpha=0.05, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "[1] Intel AI Academy, Machine Learning 501. <br>\n",
    "[2] A. Geron (2017), Hands-on machine learning with Scikit-Learn and TensorFlow (O’Reilly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
