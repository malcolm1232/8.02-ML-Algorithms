{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# k-Nearest Neighbours\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn to apply the k-Nearest Neighbours (k-NN)\n",
    "- Implement a k-NN algorithm to find the nearest neighbour.\n",
    "- Use Scikit-Learn in-built library to perform k-NN search.\n",
    "- Use k-NN for imputation.\n",
    "- Use k-NN for classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The k-Nearest Neighbour (k-NN) is a widely used algorithm that can be applied to many different problems. We can use k-NN for classification or imputing missing values in our dataset. The idea behind the algorithm is intuitive and one that we uses instinctively - we simply assume things that are similar to each other works in a similar manner.\n",
    "\n",
    "We will see how we can apply the k-NN algorithm to a set of data for imputing missing values and classification.\n",
    "\n",
    "We will first write the codes from scratch to have a better understanding of the algorithm and subsequently use Scikit-Learn for efficiency and easy of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Using Python and Numpy\n",
    "\n",
    "We will now implement k-NN using Python with the help of Numpy. The implementation is to ensure that you have good grasp of the concept behind k-NN. In this implementation, we will use Euclidean distance as a measurement of the similarity among data points. To find the nearest neighbour of a data point, we will calculate the distances to all other data points and choose the one that is nearest (with the shortest distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The variable data is a numpy array holding a 2-dimensional dataset\n",
    "import numpy as np\n",
    "data = np.array([\n",
    "    [0, 10],\n",
    "    [1, 9],\n",
    "    [2, 2],\n",
    "    [3, 5],\n",
    "    [4, 15],\n",
    "    [5, 9],\n",
    "    [7, 1],\n",
    "    [8, 8],\n",
    "    [9, 4]\n",
    "    ])\n",
    "\n",
    "#We are looking for the nearest neighbour of the search point [5, 2]\n",
    "searchpoint = np.array([[6,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the data by generating a scatter plot\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "#Create a figure\n",
    "plot.figure()\n",
    "plot.title(\"Plot of X vs Y\")\n",
    "plot.xlabel(\"x\")\n",
    "plot.ylabel(\"y\")\n",
    "plot.scatter(data[:,0], data[:,1], c=\"red\")\n",
    "\n",
    "plot.scatter(searchpoint[:, 0], searchpoint[:, 1], c=\"blue\", marker=\"x\")\n",
    "plot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dots are our data points and the blue “x” symbol is our search point. By visual inspection, we know that [7,1] should be our nearest neighbour.\n",
    "\n",
    "Let us get the machine to do the searching for us using a k-NN algorithm.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbour=[7 1]\n",
      "Distance=1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "#Calculates distances from all points to the search point\n",
    "#note that axis = 1 means sum by rows\n",
    "distances = np.sqrt(np.sum((data - searchpoint)**2, axis=1)) \n",
    "\n",
    "#The function argmin calculates the index of the array element with the \n",
    "#smallest value and in this case, the shortest distance\n",
    "index_nearest_neighbour = np.argmin(distances)\n",
    "\n",
    "#Print out the nearest neighbour, you should get [7, 1]\n",
    "print(\"Nearest Neighbour={}\".format(data[index_nearest_neighbour, :]))\n",
    "#Print out the distance\n",
    "print(\"Distance={}\".format(distances[index_nearest_neighbour]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN with Scikit-Learn\n",
    "\n",
    "Usually we do not have to implement algorithms ourselves, we can use open source libraries with more efficient and proven implementations. Let us now see how we can use scikit-learn to perform k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Scikit-learn's k-NN alogrithm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors as knn\n",
    "\n",
    "data = np.array([\n",
    "    [0, 10],\n",
    "    [1, 9],\n",
    "    [2, 2],\n",
    "    [3, 5],\n",
    "    [4, 15],\n",
    "    [5, 9],\n",
    "    [7, 1],\n",
    "    [8, 8],\n",
    "    [9, 4]\n",
    "    ])\n",
    "\n",
    "searchpoint = np.array([[6,2]])\n",
    "\n",
    "#As in standard scikit-learn operations, we first call the fit function\n",
    "#with our data. The number 1 means we are only looking for 1 neighbour. Also\n",
    "#See the next section for explanation on the algorithm=”brute” parameter\n",
    "model = knn(n_neighbors=1, algorithm=\"brute\").fit(data)\n",
    "#Call the kneighbors function to begin the search for the nearest neighbour \n",
    "distance, indices = model.kneighbors(searchpoint)\n",
    "#The function will return the index of the nearest neighbour as well as the\n",
    "#distance between the two points\n",
    "print(distance)\n",
    "print(data[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Algorithm=Brute vs kd_tree vs ball_tree\n",
    "\n",
    "The algorithm parameter indicates how we want the k-NN algorithm to search for the nearest neighbour. If we specify  ```brute```, it uses brute force method by calculating the distance between the search point and all the data points as we have done previously in our own implementation. However, this method is inefficient and is computational intensive with large number of data points and features. In such cases, we should switch to either ```kd_tree``` or ```ball_tree```. \n",
    "\n",
    "We can also indicate ```auto``` for the algorithm parameter. In this case, we leave it to Scikit-Learn to figure out which is the best method to use. Note that if the algorithm parameter is not specified, it is defaulted to using ```auto```.\n",
    "Refer to http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms for a detailed discussion of the algorithms and their merits.\n",
    "\n",
    "## k-NN Imputation\n",
    "\n",
    "A common use of k-NN is for imputing missing values in a data set. The idea is that when there is a missing value in a data sample, we can replace the missing values with one from a similar sample. We find the nearest neighbour and use the value from the nearest neighbout as a substitutional value.\n",
    "\n",
    "Let us see how that works. \n",
    "\n",
    "Make sure that you have downloaded the _SmokersMissingValues.csv file_. Save the file in the same directory as your jupyter notebook file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      16 and Over  16-24  25-34  35-49  50-59  60 and Over  Method_Unweighted  \\\n",
      "Year                                                                            \n",
      "1974           51     47     55     55   53.0         44.0                  1   \n",
      "1976           46     43     48     50   50.0         40.0                  1   \n",
      "1978           44     40     48     48   48.0         38.0                  1   \n",
      "1980           42     38     47     45   47.0         36.0                  1   \n",
      "1982           38     36     40     40   42.0         33.0                  1   \n",
      "\n",
      "      Method_Weighted  \n",
      "Year                   \n",
      "1974                0  \n",
      "1976                0  \n",
      "1978                0  \n",
      "1980                0  \n",
      "1982                0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors as knn\n",
    "\n",
    "df = pd.get_dummies(pd.read_csv(\"C:/Users/malco/Desktop/OTHER DATA/npp/SmokersMissingValues.csv\", index_col=\"Year\"))\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use k-NN imputer offered by Scikit-Learn to automatically find the nearest neighbour and use the values from the neighbour as substitution.\n",
    "\n",
    "*Note that k-NN imputer in Scikit-Learn is a recent addition, it is only available from version 0.22 and above]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16 and Over</th>\n",
       "      <th>16-24</th>\n",
       "      <th>25-34</th>\n",
       "      <th>35-49</th>\n",
       "      <th>50-59</th>\n",
       "      <th>60 and Over</th>\n",
       "      <th>Method_Unweighted</th>\n",
       "      <th>Method_Weighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      16 and Over  16-24  25-34  35-49  50-59  60 and Over  Method_Unweighted  \\\n",
       "Year                                                                            \n",
       "1988         33.0   33.0   37.0   37.0   32.0         29.0                1.0   \n",
       "2007         22.0   27.0   29.0   25.0   20.0         13.0                0.0   \n",
       "\n",
       "      Method_Weighted  \n",
       "Year                   \n",
       "1988              0.0  \n",
       "2007              1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Create a new Imputer with k=1\n",
    "#We will only use the nearest neighbour\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "#The fit_transform will return a numpy array\n",
    "#with all values filled in, we replace our dataframe values with that \n",
    "#from the imputer\n",
    "df[:] = imputer.fit_transform(df)\n",
    "#print out and take a look at the result\n",
    "df.loc[[1988, 2007]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that results from using k-NN and interpolation may not get the same results as they used different algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16 and Over</th>\n",
       "      <th>16-24</th>\n",
       "      <th>25-34</th>\n",
       "      <th>35-49</th>\n",
       "      <th>50-59</th>\n",
       "      <th>60 and Over</th>\n",
       "      <th>Method_Unweighted</th>\n",
       "      <th>Method_Weighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      16 and Over  16-24  25-34  35-49  50-59  60 and Over  Method_Unweighted  \\\n",
       "Year                                                                            \n",
       "1988           33     33     37     37   32.0         29.0                  1   \n",
       "2007           22     27     29     25   23.0         13.0                  0   \n",
       "\n",
       "      Method_Weighted  \n",
       "Year                   \n",
       "1988                0  \n",
       "2007                1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying out interpolation using the nearest method.\n",
    "df = pd.get_dummies(pd.read_csv(\"C:/Users/malco/Desktop/OTHER DATA/npp/SmokersMissingValues.csv\", index_col=\"Year\"))\n",
    "df = df.interpolate(method=\"nearest\")\n",
    "df.loc[[1988, 2007]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Before version 0.22, sckit-learn perform missing values imputation using mean, median or most frequent values but it does not support k-NN imputation. The k-NN imputation above only works for version 0.22 onwards, if you get an error, make sure to update your scikit-learn package.\n",
    "2. Alterative implementation are also available as add-on https://github.com/scikit-learn/scikit-learn/issues/2989 and fancyimpute (https://pypi.org/project/fancyimpute/) instead.\n",
    "3. Pandas also support operations to impute missing values, but it also does not support k-NN. It does support fill-forward, fill-backward and interpolation which are very useful for data that has certain trends. Alternatively, you can also replace missing values with fixed or mean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Classifier\n",
    "\n",
    "k-NN can also be used to implement a classifier. Since Scikit-Learn already provided us with a k-NN classifier, our job is much easier.\n",
    "We will use a simple data set ```churn_classifier.csv``` file to illustrate the use of the k-NN classifier. \n",
    "\n",
    "## Step 9\n",
    "\n",
    "Download the file ```churn_classifier.csv``` and place it in the same directory as your notebook file.\n",
    "\n",
    "## Step 10\n",
    "\n",
    "Place the following codes in a new cell. The comments provide the explanation of how the classifier is trained and used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LONGDIST  International     LOCAL  DROPPED  AGE  SEX  STATUS  CHILDREN  \\\n",
      "Id                                                                           \n",
      "0    5.24640        7.51510  86.32780        0   57    0       1         2   \n",
      "3    0.00000        0.00000   3.94229        0   50    0       0         2   \n",
      "4    5.55564        0.00000   9.36347        1   68    0       1         2   \n",
      "8   14.01930        5.68043  29.80650        0   34    1       0         0   \n",
      "10  13.66400        2.95642  32.63810        0   60    1       1         2   \n",
      "\n",
      "    Est_Income  Car_Owner  CHURNED  \n",
      "Id                                  \n",
      "0      27535.3          1        1  \n",
      "3      64632.3          0        1  \n",
      "4      81000.9          0        1  \n",
      "8      87467.1          1        0  \n",
      "10     83220.6          0        1  \n",
      "Customer likely to terminate contract\n",
      "Customer not likely to terminate contract\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "\n",
    "#We need to import the k-NN Classifier from skleart.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Read in the CSV file, set the Id column as the index\n",
    "df = pd.read_csv(\"C:/Users/malco/Desktop/OTHER DATA/npp/churn_classifier.csv\", index_col=\"Id\")\n",
    "#Print out the first 10 rows for visual inspection\n",
    "print(df.head(5))\n",
    "\n",
    "#Set the CHURNED column as our label (target value to be predicted)\n",
    "YTrain = df[\"CHURNED\"]\n",
    "#Remove the CHURNED column from our input variables\n",
    "XTrain = df.drop(\"CHURNED\", axis=1)\n",
    "#Create a k-NN Classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "#Train the classifier using our training data\n",
    "classifier.fit(XTrain, YTrain)\n",
    "\n",
    "#Let us test, note that our testing data is similar to row with id 0 and 8\n",
    "#Expected result is 1 (likely) and 0 (unlikely)\n",
    "XTest = pd.DataFrame([\n",
    "[5.2, 7.5, 80.1, 0, 53, 0, 1, 1, 23000.00, 1],\n",
    "[15.0, 4.5, 30.1, 0, 35, 1, 0, 0, 90000.00, 1]\n",
    "])\n",
    "\n",
    "#Iterate through each row and do the predicton\n",
    "for index, row in XTest.iterrows():\n",
    "    if classifier.predict(row.values.reshape(1, -1))[0] == 0:\n",
    "        print(\"Customer not likely to terminate contract\")\n",
    "    else:\n",
    "        print(\"Customer likely to terminate contract\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The practical demonstrate the basic concept of the k-NN algorithm. We also illustrated the common application of the algorithm in imputation and classification using the tools provided by Scikit-Learn and Pandas."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
