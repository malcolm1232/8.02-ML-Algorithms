{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Association Rules\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn to use Python and _mlxtend_ (Machine Learning Extension) to generate frequent itemsets from a set of transactions data.\n",
    "- Use the mlxtend library to mine for association rules from frequent itemsets.\n",
    "- Apply Association Rules mining to unstructured text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When people buy cigarettes, do they tend to also buy chocolate or beer? If people have high cholesterol, do they also tend to have high blood pressure? If people buy car insurance, do they also buy house insurance? \n",
    "Answers to such questions can form the basis of brand positioning, advertising and even direct marketing. But how do we find out if such associations exist? And how can we search for them when our databases have tens of thousands of records and many fields? \n",
    "\n",
    "Association detection algorithms provide rules describing the values of fields that typically occur together. They can therefore be used as an approach to this area of data understanding. \n",
    "\n",
    "An association rule has two parts, an antecedent (if) and a consequent (then). An antecedent is an item found in the data. A consequent is an item that is found in combination with the antecedent.\n",
    "\n",
    "Below are some common terms used when working on association rules:\n",
    "- *Instances* indicates the number of records in the data set that match the antecedents. \n",
    "- *Support* refers to the percentage of records that match the antecedents. (Same as “Instances” but in percentage)\n",
    "- *Confidence* is the percentage of all records matching the antecedents that also match the consequent. \n",
    "- *Rule Support* is the percentage of records that match the entire rule (both the antecedents and consequent). \n",
    "- *Lift* refers to the expected return using a model or rule. In this context it is the ratio of the rule confidence to the overall percentage occurrence of the consequent in the data. Think of it as a measure of how much better the model is compared with a random-choice model.\n",
    "\n",
    "We have been using Sckit-Learn for our other machine learning tasks so far. Unfortunately, Scikit-Learn does not offer any support for frequent itemset generation nor association rule mining. So in this practical, we will be using the mlxtend (Machine Learning Extensions) package. The package is  available at http://rasbt.github.io/mlxtend/. The python extension offers tools that helps in “day-to-day data science tasks”.\n",
    "\n",
    "To install mlxtend, issue the following command in a conda command prompt:\n",
    "\n",
    "```\n",
    "conda install mlxtend --channel conda-forge\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Frequent Itemsets\n",
    "\n",
    "Let us now see how we can generate frequent itemsets given a set of transactions records.\n",
    "\n",
    "### Step 1 Loading the Data File\n",
    "\n",
    "Obtain a copy of the ```Shopping.csv``` file and save it in the same directory as your jupyter notebook file. \n",
    "\n",
    "Add codes to read in the ```Shopping.csv``` file using *Pandas*. Print out the data to see the dataset.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Click here to view codes</strong></summary>\n",
    "\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Shopping.csv\")\n",
    "df.head()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Codes here to import pandas and load Shopping.csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following shows the printout of the shopping data:\n",
    "\n",
    "```\n",
    "Ready made Frozen foods Alcohol Fresh Vegetables Milk Bakery goods Fresh meat Toiletries Snacks Tinned Goods\n",
    "...\n",
    "\n",
    "0   1   0   0   0   0   0   0   0   1   0\n",
    "1   1   0   0   0   0   0   0   1   0   0\n",
    "2   1   0   0   0   0   0   0   1   1   0\n",
    "3   1   0   0   0   1   1   0   1   1   0\n",
    "4   1   0   0   0   0   0   0   1   1   0\n",
    "```\n",
    "The data consists of rows of transactions. Each column indicates if an item has been bought in the transaction, a value of 0 means _no_ while 1 means _yes_. However, this is not the format expected by the *mlxtend* library, we need to have ```True``` and ```False``` values instead of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify your codes to add a ```dtype=bool``` parameter to ```read_csv()``` function call as follows:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"Shopping.csv\", dtype=bool)\n",
    "```\n",
    "\n",
    "Execute them again with the ```dtype=bool``` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should now see the following output:\n",
    "\n",
    "```\n",
    "Ready made Frozen foods Alcohol Fresh Vegetables  Milk Bakery goods Fresh meat Toiletries Snacks Tinned Goods\n",
    "...\n",
    "0   True   False   False   False   False   False   False   False   False   False\n",
    "1   True   False   False   False   False   False   False   True    True    False\n",
    "2   True   False   False   False   False   False   False   True    True    False\n",
    "3   True   False   False   False   False   True    False   True    True    False\n",
    "4   True   False   False   False   False   False   False   True    True    False\n",
    "```\n",
    "\n",
    "The values of 0 and 1 has been changed to ```False``` and ```True```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to generate the frequent itemsets.\n",
    "\n",
    "Add the following lines to use the _Apriori_ algorithm to generate the frequent itemsets with minimum support level of 10%\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "```\n",
    "\n",
    "Recall that frequent itemsets are determined by the specified _support_ value. We stated a value of 0.1 (10%). So  itemsets with  support of less than 10% will be discarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your codes here to generate the frequent itemset using mlxtend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the codes, you will see the frequent itemsets generated by the ```apriori()``` function as follows:\n",
    "\n",
    "         support                                           itemsets\n",
    "    0   0.492366                                       (Ready made)\n",
    "    1   0.402036                                     (Frozen foods)\n",
    "    2   0.394402                                          (Alcohol)\n",
    "    3   0.188295                                             (Milk)\n",
    "    4   0.428753                                     (Bakery goods)\n",
    "    5   0.474555                                           (Snacks)\n",
    "    6   0.455471                                     (Tinned Goods)\n",
    "    7   0.211196                         (Ready made, Frozen foods)\n",
    "    8   0.212468                              (Alcohol, Ready made)\n",
    "    9   0.133588                                 (Ready made, Milk)\n",
    "    10  0.255725                         (Ready made, Bakery goods)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listing shows the frequent itemsets together with the support of the itemset. We requested for a support of 10%, from the list, we see that there are a total of 52 itemsets with support of 10% or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Association Rules from Frequent Itemsets\n",
    "\n",
    "Once we have the frequent itemsets, we can proceed to generate the association rules. Whilst we generate the frequent itemsets based on the desired __support__ value, association rules are generated from the frequent itemsets (to ensure wide applicability) based on __confidence__ value.\n",
    "\n",
    "Let us now generate rules based on a confidence of 0.7 (70%)\n",
    "\n",
    "Add and run the following codes:\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "print(association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7))\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your codes here to generate rules from the frequent itemsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following outputs:\n",
    "\n",
    "                                     antecedents     consequents  \n",
    "    0                                     (Milk)    (Ready made)   \n",
    "    1                                     (Milk)  (Bakery goods)   \n",
    "    2                         (Ready made, Milk)  (Bakery goods)   \n",
    "    3                       (Milk, Bakery goods)    (Ready made)   \n",
    "    4               (Frozen foods, Tinned Goods)  (Bakery goods)   \n",
    "    5                    (Alcohol, Tinned Goods)  (Bakery goods)   \n",
    "    6                       (Milk, Tinned Goods)  (Bakery goods)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    antecedent support  consequent support   support  confidence      lift  \n",
    "    0             0.188295            0.492366  0.133588    0.709459  1.440918   \n",
    "    1             0.188295            0.428753  0.139949    0.743243  1.733499   \n",
    "    2             0.133588            0.428753  0.105598    0.790476  1.843663   \n",
    "    3             0.139949            0.492366  0.105598    0.754545  1.532488   \n",
    "    4             0.207379            0.428753  0.148855    0.717791  1.674137   \n",
    "    5             0.173028            0.428753  0.123410    0.713235  1.663510   \n",
    "    6             0.127226            0.428753  0.100509    0.790000  1.842552   \n",
    "    \n",
    "    \n",
    "        leverage  conviction  \n",
    "    0   0.040878    1.747204  \n",
    "    1   0.059217    2.224856  \n",
    "    2   0.048322    2.726405  \n",
    "    3   0.036692    2.068137  \n",
    "    4   0.059940    2.024201  \n",
    "    5   0.049223    1.992040  \n",
    "    6   0.045960    2.720223 \n",
    "    \n",
    "    \n",
    "For the first rules with ID 0, we get ```(Milk)``` &#8658; ```(Ready made)```.\n",
    "\n",
    "**Support and Confidence**\n",
    "\n",
    "It also shows the support to to be ```0.133588``` and confidence of ```0.709459```.\n",
    "\n",
    "### Other Measures\n",
    "\n",
    "Besides the usual, support and confidence, we can also use other measures like *lift*, *leverage* and *conviction* to help us better understand the rules that were generated. These values are generated for us by default.\n",
    "\n",
    "**Lift**\n",
    "\n",
    "The lift value is ```1.440918``` which is positive. The *lift* value provides an indication of how the consequents  and antecedents are dependent on each other.\n",
    "- If the value is < 1, the two is negatively dependent, meaning if one occurs, the other is less likely to occur (subtitution effect).\n",
    "- If lift = 1, then they are independent of each other, which in turn means that the rule is useless. \n",
    "- Lastly, if lift > 1, then they are positively dependent and occurrence of one means higher chance of the other occurring (complementary effect).\n",
    "\n",
    "The higher a lift is, the better the rule is. For association rules, the lift should be > 1.\n",
    "\n",
    "\n",
    "**Leverage**\n",
    "\n",
    "We also see another measure which is the *leverage*. *Leverage* is similar to *lift* in that it compares the rules to the case where the antecedents and consequents are independent. In this case a leverage of 0 means that the antecendent and consequent itemsets are independent. For example, ```(Milk)``` &#8658; ```(Ready made)``` has a leverage value of 0.04 (>0) which means that the two products sells more together then can be expected if they were sold independently.\n",
    "\n",
    "\n",
    "**Conviction**\n",
    "\n",
    "Conviction is a measure that takes into account the _support_ of the antecedent and _confidence_ of the rule in a single measure. The value ranges from 0 to infinity. The higher the value, the more the consequent is dependent on the antecedent. In short, the larger the value, the better.\n",
    "\n",
    "The value for the rule ```(Milk)``` &#8658; ```(Ready made)``` is ```1.747204```.\n",
    "\n",
    "We will not discuss lift, leverage and conviction in details, if you are interested, please refer to [http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Run the apriori function again with 20% support, how many frequent itemsets are generated?\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        <strong>Click for answer</strong>\n",
    "    </summary>\n",
    "\n",
    "```\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "```\n",
    "    \n",
    "You should see that the number of frequent itemsets is 19.\n",
    "  \n",
    "</details>\n",
    "    \n",
    "2. With these itemset for a support of 20%, generate rules based on a confidence threshold of 0.7, how many rules are generated?\n",
    "    \n",
    "<details>\n",
    "    <summary>\n",
    "        <strong>Click for answer</strong>\n",
    "    </summary>\n",
    "    \n",
    "```\n",
    "print(association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7))\n",
    "```    \n",
    "    \n",
    "There are no rules that can be generated from the frequent itemsets. In other words, no rules can be generated from the current transactional data with support of > 20% and confidence of > 0.7.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your exercise codes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Associations\n",
    "\n",
    "Besides analysing things people buy together, we can also apply association rules to other cases like finding out words that appears frequently together, from such associations, we can gain more insights in a corpus (collection of text).\n",
    "\n",
    "Let us try to apply association rule mining to a set of text and see what we can find.\n",
    "\n",
    "### Step 1 Loading the Attractions.txt File\n",
    "\n",
    "Obtain the file *Attractions.txt* and load the file using Pandas\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Click to see codes</strong></summary>\n",
    "    \n",
    "    \n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Attractions.txt\")\n",
    "df.head()\n",
    "```\n",
    "</details>\n",
    "\n",
    "Print out using the ```head()``` function to see the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter codes here to read in the Attractions.txt file using Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following:\n",
    "\n",
    "     \ttext\n",
    "    0 \tgarden bay place wish aside entire cheap worth...\n",
    "    1 \tgarden bay beautiful walk hours place guided e...\n",
    "    2 \thighlights visit singapore flower singapore cl...\n",
    "    3 \tloved garden door waterfall garden bay flower ...\n",
    "    4 \tbeautiful recommend flower dome cloud dome gar...\n",
    "\n",
    "\n",
    "As can be seen from the snapshot, it contains reviews about an attraction (Gardens by the Bay to be specific) in Singapore.\n",
    "\n",
    "Each line is a single review and text pre-processing (like removing of stop words) has been performed on the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Configuring Natural Language Toolkit (NLTK)\n",
    "\n",
    "We will need the ```nltk``` package to process the text data. \n",
    "\n",
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"- https://www.nltk.org/\n",
    "\n",
    "You will first need to install it if it is not already install on your machine. The conda command to install the toolkit is\n",
    "\n",
    "```conda install -c anaconda nltk ```\n",
    "\n",
    "Once installed, you will still need to install additional nltk files before it can be used, the following codes will download the files that are required. \n",
    "\n",
    "Execute the following codes:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute your codes here to download additional files for NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that files are being downloaded (or updated):\n",
    "\n",
    "```\n",
    "    [nltk_data] Downloading collection 'popular'\n",
    "    [nltk_data]    | \n",
    "    [nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
    "    [nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
    "    [nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
    "    [nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
    "    ...\n",
    "    [nltk_data]  Done downloading collection popular\n",
    "```\n",
    "It might take some time to download the required files depending on your network connection speed.\n",
    "\n",
    "### Step 3 Apply Word Tokenization\n",
    "\n",
    "We need to read in each review and break down the text into individual words. Each word is now an \"item\" in our market basket.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "df = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "print(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your codes here to tokenize text into words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the text is down converted into arrays of words as shown below:\n",
    "\n",
    "    0     [garden, bay, place, wish, aside, entire, chea...\n",
    "    1     [garden, bay, beautiful, walk, hours, place, g...\n",
    "    2     [highlights, visit, singapore, flower, singapo...\n",
    "    3     [loved, garden, door, waterfall, garden, bay, ...\n",
    "    4     [beautiful, recommend, flower, dome, cloud, do...\n",
    "    5     [singaporean, garden, garden, bay, walk, visit...\n",
    "    6     [garden, showpiece, horticulture, garden, arti...\n",
    "    7     [garden, spectacular, night, take, walk, great...\n",
    "    ...\n",
    "    \n",
    "### Step 4 Convert to Transactional Format\n",
    "\n",
    "The ```apriori()``` function works on transactional format, not an array of text, so let us now convert it to the required format with the following codes:\n",
    "\n",
    "```python\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(df).transform(df)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your codes here to encode transaction data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following output\n",
    "\n",
    "        activity  afternoon  allows  amaze  amount  ample  areas  arranged  \\\n",
    "    0      False      False   False  False   False  False  False     False   \n",
    "    1      False      False   False   True   False  False  False     False   \n",
    "    2      False      False   False   True   False  False  False     False   \n",
    "    3      False      False   False  False   False  False  False     False   \n",
    "    4      False      False   False  False   False  False  False     False   \n",
    "    5      False      False   False  False   False  False  False     False \n",
    "\n",
    "\n",
    "Recall that in a market basket analysis we use earlier, a transaction table is as follows:\n",
    "\n",
    "- rows = transactions\n",
    "- columns = items in the market\n",
    "- True = item present in the transaction\n",
    "- False = item not present in the transaction.\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "- rows = reviews\n",
    "- columns = a word in the corpus\n",
    "- True = word occurs in the document\n",
    "- False = word does not occurs in the document\n",
    "\n",
    "### Step 5 Frequent Itemsets and Assoication Rule Mining\n",
    "\n",
    "As we have done before, we will now apply the _Apriori_ algorithm to generate the frequent itemsets as well as mine the association rules. We use a minimum support of 0.4 and confidence threshold of 0.7.\n",
    "\n",
    "Enter and run the following codes:\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "print(df.columns.tolist())\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "print(frequent_itemsets.sort_values(\"support\", ascending=False))\n",
    "print(association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)) \n",
    "```\n",
    "\n",
    "The code performs the rules mining as well as print out the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your codes here to generate frequent itemsets and association rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Words in Corpus**\n",
    "\n",
    "If you run the codes, you will first see an output printing the columns which is the same as the unique words in the corpus.\n",
    "\n",
    "    ['activity', 'afternoon', 'allows', 'amaze', 'amount', 'ample', 'areas', 'arranged', 'arrangements', 'artificial', 'artistry', 'aside', 'attractions', 'audio', 'barely', 'barrage', 'bay', 'beautiful', 'bite', 'blossom', 'break', 'breathtaking', 'bridge', 'buds', 'built', 'busy', 'cafes', 'calm', 'canalized', 'captions', 'chance', 'cheap', 'cheers', 'cherry', 'christmas', 'city', 'climate', 'close', 'cloud', 'come', 'compared', 'completing', 'complex',\n",
    "    ...\n",
    "\n",
    "It is a result from the statement\n",
    "```python\n",
    "print(df.columns.tolist())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Frequent Itemsets**\n",
    "\n",
    "You will next see the frequent itemsets from the statement \n",
    "\n",
    "```python\n",
    "print(frequent_itemsets.sort_values(\"support\", ascending=True))\n",
    "```\n",
    "\n",
    "```\n",
    "     support                  itemsets\n",
    "5   0.933333                  (garden)\n",
    "1   0.733333                     (bay)\n",
    "12  0.733333             (bay, garden)\n",
    "4   0.533333                  (flower)\n",
    "11  0.466667             (bay, flower)\n",
    "14  0.466667       (beautiful, garden)\n",
    "2   0.466667               (beautiful)\n",
    "20  0.466667     (bay, garden, flower)\n",
    "19  0.466667  (beautiful, bay, garden)\n",
    "10  0.466667          (beautiful, bay)\n",
    "15  0.466667          (flower, garden)\n",
    "18  0.400000            (garden, walk)\n",
    "17  0.400000           (visit, garden)\n",
    "16  0.400000       (singapore, garden)\n",
    "0   0.400000                   (amaze)\n",
    "13  0.400000              (bay, visit)\n",
    "9   0.400000           (amaze, garden)\n",
    "8   0.400000                    (walk)\n",
    "7   0.400000                   (visit)\n",
    "6   0.400000               (singapore)\n",
    "3   0.400000                    (dome)\n",
    "21  0.400000      (bay, visit, garden)\n",
    "```\n",
    "    \n",
    "The frequent itemsets tell us which are the words that occur frequently together. For example, we see the words \"amaze\" and \"garden\" appearing together with support of 0.4 (40%). Also the words {\"bay\", \"garden\"} has support of 73.33% which is not suprising as the reviews are on Gardens by the Bay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assoication Rules**\n",
    "\n",
    "\n",
    "You should also see a set of rules\n",
    "\n",
    "                antecedents    consequents  antecedent support  \n",
    "    0               (amaze)       (garden)            0.400000   \n",
    "    1           (beautiful)          (bay)            0.466667   \n",
    "    2              (flower)          (bay)            0.533333   \n",
    "    3                 (bay)       (garden)            0.733333   \n",
    "    4              (garden)          (bay)            0.933333   \n",
    "    5               (visit)          (bay)            0.400000\n",
    "\n",
    "The rules suggest the association between set of words.\n",
    "\n",
    "        consequent support   support  confidence      lift  leverage  conviction  \n",
    "    0             0.933333  0.400000    1.000000  1.071429  0.026667         inf  \n",
    "    1             0.733333  0.466667    1.000000  1.363636  0.124444         inf  \n",
    "    2             0.733333  0.466667    0.875000  1.193182  0.075556    2.133333  \n",
    "    3             0.933333  0.733333    1.000000  1.071429  0.048889         inf  \n",
    "    4             0.733333  0.733333    0.785714  1.071429  0.048889    1.244444  \n",
    "    5             0.733333  0.400000    1.000000  1.363636  0.106667         inf \n",
    "\n",
    "As expected, we can see that the rule ```(bay)``` &#8658; ```(garden)``` has a 100% confidence.\n",
    "\n",
    "Other associated words like {\"amaze\"} &#8658; {\"garden\"} has high confidence of  100% and support of 40%. This tells us that many reviews use the word \"amaze\" with \"garden\" which leads us to suspect that the reviews are pretty good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical, we looked at how to generate frequent itemsets and mine association rules from the frequent itemsets. We also look at the measures like support, confidence, lift, leverage and conviction.  Lastly, we did an exercise on a set of review text for a tourist attraction. The exercise provides us with a very good idea of the words that are associated with each other."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
